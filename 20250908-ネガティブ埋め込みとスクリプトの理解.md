>ネガ埋め込みを使うとき ネガ埋め込みは持っていません、どこでどうやって得られますか？ >行っていること：既存モデル（SD1.5 + 任意のネガ埋め込み）での画像翻訳（推論） ネガ埋め込みを使わないならSD1.5のみとなりますがこれはただエンコード+デコードしているだけでほぼ変わらない画像がコピーされるという理解で良いですか？論文の要点を再現できていない実行ということですか。

---

ナイス切り分け。結論からいきます。

# 1) ネガ埋め込み（negative embedding）はどこでどうやって入手する？

作者配布がなければ**自分で Textual Inversion を学習して作る**のが基本です。幸い、このリポジトリには **synthetic な衣服画像（`synfashion_release`）** が入っているので、これを “レンダーっぽさ（合成感）＝除去したい特徴” として学習すれば、そのまま **ネガティブ領域の埋め込み**になります。

### 手順（最短ルート）

1. 画像フォルダを用意
   `./data/neg_fashion/` に、合成（レンダー）寄りの画像を200〜1000枚ほど置く
   （まずは `synfashion_release` から20〜100枚で試してOK。最終的には多いほど安定）

2. Textual Inversion で学習（Diffusers 付属のスクリプトを使用）

```bash
pip install accelerate
accelerate config  # 初回だけ対話設定（単GPUならデフォルトでOK）

accelerate launch \
  -m diffusers.examples.textual_inversion.textual_inversion \
  --pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \
  --instance_data_dir=./data/neg_fashion \
  --placeholder_token="<neg_fashion>" \
  --initializer_token="style" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --max_train_steps=1000 \
  --learning_rate=5e-04 \
  --output_dir=./neg_embedding
```

3. できあがるもの
   `./neg_embedding/learned_embeds.bin`（または `.pt`） が出力されます。

4. 使い方（今回のスクリプト）

```bash
--use_negEmbedding \
--negative_embedding_dir=./neg_embedding \
--negative_prompt="<neg_fashion>"
```

※ `negative_prompt` は Textual Inversion 学習時に決めた **placeholder token** を渡します（上記例では `<neg_fashion>`）。

> 補足
>
> * まずは 200〜500枚・1k〜2k steps くらいで充分“効き”ます。
> * GPU が厳しければ `--learning_rate=1e-3` に上げて step を減らす等、短時間チューニングでも可。
> * より堅牢にしたければ、レンダーの“失敗例・荒れた質感・シワの出方”に寄ったセットを混ぜると効果的。

---

# 2) `Realistic_translation.py` は学習？推論？出力は何？

これは**学習ではなく推論（変換）**です。最終成果物は**画像ファイル**（モデルを保存しません）。

ざっくり動作：

1. **SD1.5 をロード**（`--model_path`）。
2. （オプション）**ネガ埋め込み**をロード（`--use_negEmbedding` など）。
3. **DDIM Inversion** で入力画像→潜在へ逆拡散（`--inversion_as_start`）。
4. \*\*TAC（Texture-preserving Attention Control）\*\*でアテンションを差し替えつつ拡散（`--attn_replace_layers`, `--replace_steps_ratio`）。
5. 生成画像を `--output_dir` 配下へ保存。

---

# ネガ埋め込み無しだとどうなる？

* 今のコマンドは `--source_prompt='' --target_prompt=''` で **プロンプト無指定**、`denoising_strength=0.3` と弱め。
* **ネガ埋め込みも無し**なので、基本は「**DDIM で潜在に落として少しだけ再構成した“ほぼ元画像”**」に近い挙動になります（わずかに劣化・揺らぎが乗ることはあります）。
* つまり\*\*論文のキモ（ネガティブ領域の抑制＋TAC による質感維持変換）\*\*はほぼ出ません。再現度は低いです。

### すぐ効果を見たいときのコツ（ネガ埋め込み無しでも）

* `--target_prompt` に「現実写真らしさ」を強める語を入れる
  例：`"a well-lit street fashion photo, DSLR, high detail, natural skin texture"`
* `--denoising_strength` を 0.45〜0.6 に上げる（変化量アップ）
* `--replace_steps_ratio` を 0.7〜0.9 で調整（置換の効かせ方）
* それでも **学習したネガ埋め込み**の効果には及びません。最終的には TI 学習が必要です。

---

## まとめ・次の一手

* **ネガ埋め込みがない→自前で Textual Inversion を学習**

  * データ：`synfashion_release` を流用して OK
  * トークン：`<neg_fashion>` などを任意に決める
  * 出力：`./neg_embedding/learned_embeds.bin`
* 使うとき：

  ```bash
  --use_negEmbedding \
  --negative_embedding_dir=./neg_embedding \
  --negative_prompt="<neg_fashion>"
  ```
* ネガ無し・空プロンプトのままだと**ほぼ再構成**なので、論文の要点は再現されません。まずは TI を軽く学習 → 効きを確認 → データとステップ数を増やして調整、の順がおすすめです。

必要なら、**学習用フォルダの用意スクリプト**（画像の自動リサイズ・フィルタリング）や、**Windows での保存ファイル名サニタイズのパッチ**もすぐ出します。
